{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ravish Chawla\n",
    "### Topic Modeling with LDA and NMF algorithms on the ABC News Headlines Dataset\n",
    "#### July 31, 2017\n",
    "#### Minor changes for Topic Modeling Workshop at Northwestern University, August, 2019.\n",
    "#### [https://github.com/nuitrcs/topic-modeling-workshop](https://github.com/nuitrcs/topic-modeling-workshop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imports\n",
    "\n",
    "We import Pandas, numpy and scipy for data structures. We use gensim for LDA, and sklearn for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We are using the ABC News headlines dataset. The data contain two columns\n",
    "separated by a comma:\n",
    "\n",
    "1. *publish_date*, the publication date for the article.\n",
    "2. *headline_text*, the text of the headline.\n",
    "\n",
    "We only heed the headline_text for analysis, so we ignore the publish_date column.\n",
    "\n",
    "Some lines are badly formatted, so we skip them by specifying error_bad_lines=False\n",
    "in the read_csv call.  If we did not, the read process would fail the first time \n",
    "a data line with the wrong number of columns appeared.\n",
    "\n",
    "The full set of headlines has 1041793 headlines along with an initial line\n",
    "containing the column names.  This takes a while to process, so for the workshop\n",
    "we'll use a 5% random sample of 52,090 headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To use all the headlines, comment out the next line which loads the sample,\n",
    "# and uncomment the subsequent line which loads the entire set of headlines.\n",
    "\n",
    "data = pd.read_csv('data/abcnews-headlines-five-percent-sample.csv', error_bad_lines=False)\n",
    "#data = pd.read_csv('data/abcnews-headlines-full.csv', error_bad_lines=False)\n",
    "\n",
    "# Both columns of the headlines data have now been read into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We only need the Headlines_text column from the data, so we grab the headline_text entries and place them in the variable data_text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['headline_text']]\n",
    "data_text = data_text.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the stopwords.   We'll use the default set from nltk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))    #stop_words now contains the list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the stopwords from each headline.  We need to tokenize the headline texts to do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(data_text)):\n",
    "    \n",
    "    #go through each word in each data_text row, remove stopwords, and set \n",
    "    #them on the index.\n",
    "    \n",
    "    data_text.iloc[idx]['headline_text'] = \\\n",
    "        [word for word in data_text.iloc[idx]['headline_text'].strip().split(' ') \\\n",
    "        if word not in stop_words and ( len(word) > 0 ) ]\n",
    "    \n",
    "    #print log to monitor output.\n",
    "    \n",
    "    if idx % 10000 == 0:\n",
    "        sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)))\n",
    "\n",
    "sys.stdout.write('\\rc = ' + str(idx + 1) + ' / ' + str(len(data_text)))\n",
    "sys.stdout.write('  Completed tokenization and stopword removal.')\n",
    "\n",
    "#At this point, data_text contains the tokenized headlines with stopwords removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all the words into a single array for input to the Latent Dirichlet Allocation algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines = [value[0] for value in data_text.iloc[0:].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll extract ten topics. You may want to experiment on your own with other values for the number of topics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "We will use the gensim library for LDA.  First, obtain a id-to-word dictionary. \n",
    "Second, for each headline, use the dictionary to obtain a mapping of the word id \n",
    "to the word count for each word in each headline. The LDA model uses both of these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = gensim.corpora.Dictionary(train_headlines)\n",
    "\n",
    "#id_to_word now contains the id-to-word dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a bag-of-words corpus from the dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id_to_word.doc2bow(text) for text in train_headlines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply latent dirichlet allocation to extract the topics in the headlines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=id_to_word, num_topics=num_topics)\n",
    "\n",
    "#lda has the generated LDA model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect top words for each extracted topic.**\n",
    "\n",
    "Iterate over the number of topics, get the top twenty words (topn) in each extracted topic, and \n",
    "add them to a dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the dataframe to show the topic words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(lda, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Non-negative matrix factorization (NMF)\n",
    "\n",
    "For NMF, we need to obtain a design matrix. To improve results, we apply a \n",
    "TfIdf transformation to the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The count vectorizer needs a list of strings, not an array, so move the headline strings into a list, **\n",
    "**and add a blank between each headline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
    "\n",
    "# train_headlines_sentences now contains the headline texts as a list of strings joined together with a \n",
    "# space separating them. \n",
    "\n",
    "# Uncomment the next line to display the list if you're interested.\n",
    "# print( train_headlines_sentences )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a Counts design matrix, for which we use SKLearn’s CountVectorizer \n",
    "module.  The transformation returns a matrix of size (Documents x Features), \n",
    "where the value of a cell is the number of times the feature (word) \n",
    "appears in that document.**\n",
    "\n",
    "**To reduce the size of the matrix, and to speed up computation, set the maximum \n",
    "feature size to 5000. That takes the top 5000 best features that can contribute \n",
    "to the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
    "\n",
    "#x_counts contains the feature counts for each headline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, create a TfIdf Transformer, and transform the counts with the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(x_counts)\n",
    "\n",
    "#x_tfidf contains the TF/IDF transformed feature counts for the words in the headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the TfIdf values to unit length for each row.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain an NMF model which we will fit with the sentences.**\n",
    "\n",
    "We use a singular value decomposition to initialize the topic extraction \n",
    "rather than a random state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=num_topics, init='nndsvd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the NMF model to the TFIDF transformed headlines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtfidf_norm)\n",
    "\n",
    "#The model now contains the results of the NMF topic extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we did for LDA, we can display the words for each extracted topic in a table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #The word ids obtained need to be reverse-mapped to the words so we can \n",
    "    #print the topic names.\n",
    "    \n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #For each topic, obtain the largest values, and add the words they map \n",
    "        #to into the dictionary.\n",
    "\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the dataframe to show the topic words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following discussion applies to the topics produced by using the 5% sample of headlines.  If you are running the full set, the results may differ.**\n",
    "\n",
    "The two tables above, in each section, show the results from LDA and NMF on both datasets. There is some coherence between the words in each clustering. For example, Topic #01 in LDA shows words associated with potentially violent incidents, such as “police”, “suicide”, and “dying”. Other topics show different patterns. \n",
    "\n",
    "On the other hand, comparing the results of LDA to NMF also shows that NMF performs better. Looking at Topic #01, we can see there are many first names clustered into the same category, along with the word “interview”. This type of headline is very common in news articles, with wording similar to “Interview with John Smith”, or “Interview with James C. on …”. \n",
    "\n",
    "We also see two topics related to violence. First, Topic #04 focuses on police related terms, such as “probe”, “missing”, “investigate”, “arrest”, and “body”. Second, Topic #02 focuses on assault terms, such as “murder”, “stabbing”, “guilty”, and “killed”. This is an interesting split between the topics because although the terms in each are very closely related, one focuses more on police-related activity, and the other more on criminal activity. Along with the first cluster which obtain first-names, the results show that NMF (using TfIdf) performs much better than LDA on this set of texts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
