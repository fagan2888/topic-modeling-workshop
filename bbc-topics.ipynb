{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with Gensim's LDA algorithm on a selection of BBC News Articles.\n",
    "#### Topic Modeling Workshop at Northwestern University, August, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle collections import warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from collections.abc import Iterable\n",
    "except ImportError:\n",
    "    from collections import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppress other annoying warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig( format='%(asctime)s : %(levelname)s : %(message)s' , \n",
    "    level=logging.ERROR )\n",
    "\n",
    "warnings.filterwarnings( \"ignore\" , category=DeprecationWarning )\n",
    "warnings.filterwarnings( action='ignore', category=UserWarning, module='gensim' )\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display progress bars.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Article Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( 'data/bbc-articles.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display number of rows (articles) and columns in data.**\n",
    "**The first column is the language of the article.**\n",
    "**The second column is the article text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the first row which contains the column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index( drop=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display number of rows and columns again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of articles in each language.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = data.articles.progress_map( detect )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The article language is specified by a two-letter ISO code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only the English language articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.language=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split articles into sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentences'] = data.articles.progress_map( sent_tokenize )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display first three sentences of first article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentences'].head(1).tolist()[0][:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize words in each sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['tokens_sentences'] = data['sentences'].progress_map(\n",
    "    lambda sentences: [ word_tokenize( sentence ) for sentence in sentences ] )\n",
    "print( data['tokens_sentences'].head(1).tolist()[0][:3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize words (with part of speech tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS_tokens'] = data['tokens_sentences'].progress_map(\n",
    "    lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a lemmatizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize each word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regroup tokens and remove stop words.\n",
    "\n",
    "**We'll use the list of stopwords built-in to nltk plus a few others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load default stopwords from nltk.\n",
    "\n",
    "my_stopwords = stopwords.words('english')\n",
    "\n",
    "# You can add additional custom stopwords inline.\n",
    "\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', \n",
    "                   'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', \n",
    "                   'also', 'copyright', 'something']\n",
    "\n",
    "my_stopwords = my_stopwords + stopwords_verbs + stopwords_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also add custom stopwords from a text file.**\n",
    "**Let's add the Buckley-Salton stopwords as an example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckley_salton_stopwords = []\n",
    "\n",
    "with open( \"data/buckley-salton-stopwords.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        buckley_salton_stopwords.extend( line.split() )\n",
    "\n",
    "my_stopwords = my_stopwords + buckley_salton_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten list of sentences of tokens to list of tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(\n",
    "    lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: \n",
    "                                    [token.lower() \n",
    "                                     for token in tokens \n",
    "                                     if token.isalpha() \n",
    "                                    and token.lower() \n",
    "                                     not in my_stopwords and \n",
    "                                     len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['tokens'].head(1).tolist()[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Analysis (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases( tokens )\n",
    "trigram_model = Phrases( bigram_model[tokens], min_count=1 )\n",
    "tokens = list( trigram_model[bigram_model[tokens]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare objects for LDA gensim implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary( tokens )\n",
    "dictionary_LDA.filter_extremes( no_below=3 )\n",
    "corpus = [dictionary_LDA.doc2bow( tok ) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed( 32767 ) # Set pseudorandom number generate seed.\n",
    "\n",
    "num_topics = 20 # We'll start by extracting 20 topics.\n",
    "\n",
    "%time lda_model = models.LdaModel( corpus, num_topics=num_topics, \\\n",
    "                                   id2word=dictionary_LDA, \\\n",
    "                                   passes=10, \\\n",
    "                                   random_state=32767 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of LDA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute perplexity, a measure of how good the model is.**  \n",
    "**The lower the perplexity value the better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Log Perplexity: ', lda_model.log_perplexity( corpus ) )\n",
    "print( 'Perplexity: ', np.exp( -1.0 * lda_model.log_perplexity( corpus ) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Coherence Score.**  \n",
    "**The higher the coherence value the better.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(\\\n",
    "    model = lda_model, corpus=corpus, texts=tokens, \n",
    "    dictionary=dictionary_LDA, coherence = 'c_v' )\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print( '\\nCoherence Score (c_v): ' , coherence_lda )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, \n",
    "                                     num_topics=num_topics, \n",
    "                                     num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocate topics to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( data.articles.loc[0][:500] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict topics for unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''Eric Tucker, a 35-year-old co-founder of a marketing company in Austin, Tex., had just about 40 Twitter followers. But his recent tweet about paid protesters being bused to demonstrations against President-elect Donald J. Trump fueled a nationwide conspiracy theory — one that Mr. Trump joined in promoting. \n",
    "\n",
    "Mr. Tucker's post was shared at least 16,000 times on Twitter and more than 350,000 times on Facebook. The problem is that Mr. Tucker got it wrong. There were no such buses packed with paid protesters.\n",
    "\n",
    "But that didn't matter.\n",
    "\n",
    "While some fake news is produced purposefully by teenagers in the Balkans or entrepreneurs in the United States seeking to make money from advertising, false information can also arise from misinformed social media posts by regular people that are seized on and spread through a hyperpartisan blogosphere.\n",
    "\n",
    "Here, The New York Times deconstructs how Mr. Tucker’s now-deleted declaration on Twitter the night after the election turned into a fake-news phenomenon. It is an example of how, in an ever-connected world where speed often takes precedence over truth, an observation by a private citizen can quickly become a talking point, even as it is being proved false.'''\n",
    "tokens = word_tokenize(document)\n",
    "topics = lda_model.show_topics(formatted=True, \n",
    "                               num_topics=num_topics, num_words=20)\n",
    "pd.DataFrame([(el[0], round(el[1],2), topics[el[0]][1]) \n",
    "              for el in lda_model[dictionary_LDA.doc2bow(tokens)]], \n",
    "             columns=['topic #', 'weight', 'words in topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at LDA results more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocate topics for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [lda_model[corpus[i]] for i in range( len(data) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_document_to_dataframe( topics_document, num_topics ):\n",
    "    res = pd.DataFrame( columns=range( num_topics ) )\n",
    "    for topic_weight in topics_document:\n",
    "        res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "    return res\n",
    "\n",
    "topics_document_to_dataframe([(9, 0.03853655432967504), \n",
    "                              (15, 0.09130117862212643), \n",
    "                              (18, 0.8692868808484044)], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create matrix of topic weights.  Documents are rows and topics are columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic = \\\n",
    "pd.concat( [topics_document_to_dataframe( topics_document, \n",
    "                                         num_topics=num_topics ) \\\n",
    "            for topics_document in topics] ) \\\n",
    "  .reset_index( drop=True ).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which documents are about topic 10?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "document_topic.sort_values( 10, ascending=False )[10].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( data.articles.loc[91][:1000] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at distribution of topics in all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set( rc={'figure.figsize':(10,20)} )\n",
    "sns.heatmap( document_topic.loc[document_topic.idxmax(axis=1).\n",
    "                                sort_values().index] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set( rc={'figure.figsize':(10,5)} )\n",
    "document_topic.idxmax( axis=1 ).value_counts().plot.bar( \\\n",
    "                                color='lightblue' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of bubble: proportional to the proportions of the topics across the N total tokens in the corpus  \n",
    "Red bars: estimated number of times a given term was generated by a given topic  \n",
    "Blue bars: overall frequency of each term in the corpus  \n",
    "-- Relevance of words is computed with a parameter lambda  \n",
    "-- Lambda optimal value ~0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare( topic_model=lda_model, corpus=corpus, \\\n",
    "            dictionary=dictionary_LDA )\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display( vis )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "429f605e71e34a868915456ee4fb20b8": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "46817b7b86ca4b65a4bd631d2e7d777f": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "8559963a3ac94b19a5b3ea6214032316": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "96323249f1884bd5b62f87c3145932df": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d9d5e9ab22a947f398aad3ed9c365fcf": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "fa80f472c5f14bcd9a7ba0befad06400": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
